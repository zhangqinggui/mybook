##Elasticsearch分析
   
全文搜索引擎会用某种算法对要建索引的文档进行分析， 从文档中提取出若干Token(词元)， 这些算法称为Tokenizer(分词器)， 这些Token会被进一步处理， 比如转成小写等， 这些处理算法被称为Token Filter(词元处理器), 被处理后的结果被称为Term(词)， 文档中包含了几个这样的Term被称为Frequency(词频)。 引擎会建立Term和原文档的Inverted Index(倒排索引)， 这样就能根据Term很快到找到源文档了。 文本被Tokenizer处理前可能要做一些预处理， 比如去掉里面的HTML标记， 这些处理的算法被称为Character Filter(字符过滤器)， 这整个的分析算法被称为Analyzer(分析器)。

- 字符过滤 Character Filter ——使用字符过滤器转变字符
- 文本切分为分词 Tokenizer ——将文本切分为单个或多个分词
- 分词过滤 Token Filter ——使用分词过滤器转变每个分词

---

Elasticsearch附带各种内置分析器,可以使用 在任何指数没有进一步的配置:

[Analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/6.5/analysis-standard-analyzer.html)

- 标准分析器
[Standard Analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/6.5/analysis-standard-analyzer.html)

标准分析器是Elasticsearch使用的默认分析器。它是分析任何语言文本的最佳通用选择。它按照[Unicode Consortium](http://www.unicode.org/reports/tr29/)的定义在单词边界上分割文本，并删除大多数标点符号。最后，它将所有项都小写。如果不为某个字段指定分析器，那么该字段就会使用标准分析器。

---
- 简单的分析器
[Simple Analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/6.5/analysis-simple-analyzer.html)

这个简单的分析器将文本拆分为除字母外的所有内容，并将术语小写。小写转换分词器。

---
- 空格分析器
[Whitespace Analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/6.5/analysis-whitespace-analyzer.html)

空格分析器在空格上分割文本。它不小写。它什么都不做，只是根据空白切分文本。

---
- 停用词分析器
[Stop Analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/6.5/analysis-stop-analyzer.html)

分析器和simple分析器很像,只是再分词流中额外的过滤了停用词。

---
- 关键词分析器
[Keyword Analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/6.5/analysis-keyword-analyzer.html)

将整个字段当作一个单独的词。

---
- 模式分析器
[Pattern Analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/6.5/analysis-pattern-analyzer.html)

允许你指定一个分词切分的模式。但是由于可能无论如何都要指定模式，通常更有意义的做法是使用定制分析器，组合现有的模式分词和所需的分词过滤器。

---
- 多语言分析器
[Language Analyzers](https://www.elastic.co/guide/en/elasticsearch/reference/6.5/analysis-lang-analyzer.html)

特定于语言的分析器可用于多种语言。他们能够考虑到特定语言的特点。例如，英语分析器提供了一组英语停止词——像and或the之类的普通单词，它们对相关性没有太大影响——它删除了这些停止词，并且能够阻止英语单词，因为它理解英语语法规则。

---
- 指纹分析器
[Fingerprint Analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/6.5/analysis-fingerprint-analyzer.html)

分析器是创建一个专家分析器 指纹可用于重复检测。

---
- 自定义分析器
[custom](https://www.elastic.co/guide/en/elasticsearch/reference/6.5/analysis-custom-analyzer.html)

如果你不能找到一个分析器适合您的需要,您可以创建一个custom分析仪相结合适当的字符过滤器,分词器,令牌过滤器。

---


- tokenizer分词器

ES内置的tokenizer列表。

| tokenizer              | logical name    | description     |
| ---                    | ---             | ---             |
|standard tokenizer      |standard         |    标准分词               |
|edge ngram tokenizer    |edgeNGram        |    侧边N元语法               |
|keyword tokenizer       |keyword          |    不分词            |
|letter analyzer         |letter           |    按单词分           |
|lowercase analyzer      |lowercase        |letter tokenizer, lower case filter|
|ngram analyzers         |nGram            |    N元语法           |
|whitespace analyzer     |whitespace       |    以空格为分隔符拆分 |
|pattern analyzer        |pattern          |    定义分隔符的正则表达式|
|uax email url analyzer  |uax_url_email    |    不拆分url和email  |
|path hierarchy analyzer |path_hierarchy   |    处理类似/path/to/somthing样式的字符串|



- token filter分词过滤器
ES内置的token filter列表。

| token filter            | logical name         | description      |
| ---                     | ---                 | ---             |
| standard filter         | standard              | 标准分词  |
| ascii folding filter    | asciifolding         |   |
| length filter           | length               | 去掉太长或者太短的   |
| lowercase filter        | lowercase            | 转成小写|
| ngram filter            | nGram                |     侧边N元语法      |
| edge ngram filter       | edgeNGram         |     N元语法        | 
| porter stem filter      | porterStem        | 波特词干算法|
| shingle filter          | shingle           | 定义分隔符的正则表达式|
| stop filter             | stop              | 移除 stop words|
| word delimiter filter   | word_delimiter    | 将一个单词再拆成子分词|
| stemmer token filter    | stemmer           |   | 
| stemmer override filter | stemmer_override  |   | 
| keyword marker filter   | keyword_marker   |   | 
| keyword repeat filter   | keyword_repeat  |   | 
| kstem filter            | kstem            |   | 
| snowball filter         | snowball         |   | 
| phonetic filter         | phonetic         | 插件 |
| synonym filter          | synonyms         | 处理同义词 |
| compound word filter    | dictionary_decompounder, hyphenation_decompounder  | 分解复合词 |
| reverse filter          | reverse          | 反转字符串 |
| elision filter          | elision          | 去掉缩略语 |
| truncate filter         | truncate         | 截断字符串 |
| unique filter           | unique           |    |
| pattern capture filter  | pattern_capture  |   |  
| pattern replace filte   | pattern_replace  | 用正则表达式替换|
| trim filter             | trim             | 去掉空格|
| limit token             | count filter limit  | 限制token数量|
| hunspell filter         | hunspell         | 拼写检查|
| common grams filter     | common_grams     |        |
| normalization filter    | arabic_normalization, persian_normalization  |            | 



 
- character filter字符过滤器
ES内置的character filter列表

|character filter               |logical name           |description      |
| ---                           | ---                   | ---             |
|mapping char filter            |mapping                |根据配置的映射关系替换字符|
|html strip char filter         |html_strip             |去掉HTML元素|
|pattern replace char filter    |pattern_replace        |用正则表达式处理字符串|




---

- 分析是通过文档字段的文本,生成分词的过程。
- 通过类型映射,每个字段都会分配一个分析器。
- 分析器是处理的链条,由一个分词器以及若干在此分析器之前的字符过滤器,在此分词器之后分词过滤器组成。
- 在字符串送到分词器之前，字符过滤器用于处理这些字符串。
- 分词器用于将字符串切分为多个分词。
- 分词过滤器用于处理分词器所产生的分词。



---
   
>Elasticsearch 从入门到跑路